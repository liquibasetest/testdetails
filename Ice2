import sys
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext

# Initialize Spark and Glue contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init('ParquetToLakeFormationTableJob')

# Specify source and target S3 paths
source_path = 's3://your-source-bucket/source-path/'  # Replace with your actual source path
database_name = 'your_database_name'  # Replace with your actual Lake Formation database name
table_name = 'your_table_name'  # Replace with your desired Glue table name
target_path = f's3://your-target-bucket/target-path/{table_name}/'  # Replace with your actual target path

# Create a DynamicFrame for the source Parquet data
source_dyf = glueContext.create_dynamic_frame.from_catalog(
    database=database_name,
    table_name='your_parquet_table'  # Replace with your actual Parquet table name
)

# Convert DynamicFrame to Spark DataFrame
source_df = source_dyf.toDF()

# Write the Spark DataFrame to S3 in Parquet format
source_df.write.mode('overwrite').parquet(target_path)

# Create a DynamicFrame for the target Glue table using Glue DataBrew
target_dyf = glueContext.create_dynamic_frame.from_catalog(
    database=database_name,
    table_name=table_name,
    transformation_ctx='target_dyf'
)

# Print the schema of the target DynamicFrame (optional)
target_dyf.printSchema()

# Commit the job
job.commit()
