import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import random
import datetime
import boto3

# Step 1: Read the Original Parquet File to understand the data types
original_table = pq.read_table('original.parquet')
original_schema = original_table.schema

# Step 2: Create a DataFrame with Partition Dates
start_date = datetime.datetime(2022, 1, 1)
end_date = datetime.datetime(2022, 1, 10)  # Adjust the end date as needed
partition_dates = pd.date_range(start=start_date, end=end_date, freq='D')
random_partition_dates = random.choices(partition_dates, k=100)  # Select 100 random partition dates

# Step 3: Generate Fake Data
fake_data = []

# Ensure each partition has at least 100,000 rows
for partition_date in random_partition_dates:
    for _ in range(100000):
        # Create a dictionary to hold the fake data based on the original schema
        fake_record = {}
        for field in original_schema:
            field_name = field.name
            field_type = field.type

            # Handle different data types
            if field_type.equals(pa.string()):
                fake_record[field_name] = f'Fake_String_{random.randint(1, 1000)}'
            elif field_type.equals(pa.int64()):
                fake_record[field_name] = random.randint(1, 1000)
            elif field_type.equals(pa.float64()):
                fake_record[field_name] = random.uniform(1.0, 1000.0)
            elif field_type.equals(pa.timestamp('ns')):
                fake_record[field_name] = datetime.datetime(2022, 1, 1) + datetime.timedelta(days=random.randint(1, 365))
            elif field_type.equals(pa.decimal128(10, 2)):
                fake_record[field_name] = round(random.uniform(1.0, 1000.0), 2)
            elif field_type.equals(pa.date32()):
                fake_record[field_name] = (datetime.date(2022, 1, 1) + datetime.timedelta(days=random.randint(1, 365))).isoformat()
            # Add more data type handling as needed

        # Include the partition_date
        fake_record['partition_date'] = partition_date
        fake_data.append(fake_record)

# Create a Pandas DataFrame from the fake data
fake_df = pd.DataFrame(fake_data)

# Step 4: Write the Fake Data to a Parquet File
fake_table = pa.Table.from_pandas(fake_df)

# Write the fake data to a local Parquet file
pq.write_table(fake_table, 'fake_data.parquet')

# Step 5: Upload the Parquet File to S3 with Partitioning
bucket_name = 'your-s3-bucket-name'
s3_key = 'path/to/destination/'

# Create partitions based on 'partition_date'
for partition_date in random_partition_dates:
    partitioned_s3_key = f'{s3_key}partition_date={partition_date.strftime("%Y-%m-%d")}/'
    s3.upload_file('fake_data.parquet', bucket_name, f'{partitioned_s3_key}customer_sub_file.parquet')

print(f'Fake data uploaded to S3 with partitioning: s3://{bucket_name}/{s3_key}')
