import csv

# Read input CSV file
input_file = 'input.csv'

# Output file path
output_file_path = 'output/generated_terraform_code.tf'

# Output Terraform code
terraform_code = """\
resource "tablename from excel" "external_table" {
  database    = var.external_db_name
  schema      = var.schema_val_name
  location    = var.ext_stg_name
  comment     = "External table"
  file_format = "TYPE = PARQUET "
"""

# Map Athena data types to Snowflake data types
data_type_mapping = {
    'INT': 'NUMBER',
    'SMALLINT': 'NUMBER',
    'BIGINT': 'NUMBER',
    'TINYINT': 'NUMBER',
    'INTEGER': 'NUMBER',
    'DECIMAL': 'NUMBER',
    'DOUBLE': 'NUMBER',
}

# Process each row in the CSV file
with open(input_file, 'r') as csvfile:
    csv_reader = csv.DictReader(csvfile)
    for row in csv_reader:
        tablename = row['tablename']
        fieldname = row['fieldname']
        datatype = row['datatype']
        partition = row['partition'] if 'partition' in row else None

        # Map Athena data type to Snowflake data type
        snowflake_data_type = data_type_mapping.get(datatype, datatype)

        # Generate Terraform code for each column
        terraform_code += f'\n  column {{\n    name = "{fieldname}"\n    type = "{snowflake_data_type}"\n'

        # Add "as" clause for partition column
        if partition:
            terraform_code += f'    as = ({partition} as string))\n'
        else:
            terraform_code += f'    as = (CAST(GET($1, \'{fieldname}\') as {datatype}))\n'

# Complete the Terraform code
terraform_code += f'}}\n'

# Write the generated Terraform code to a file
with open(output_file_path, 'w') as output_file:
    output_file.write(terraform_code)

print(f'Terraform code written to {output_file_path}')
