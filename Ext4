import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import random
import datetime
import boto3

# Step 1: Read the Original Parquet File to understand the data types
original_table = pq.read_table('original.parquet')
original_schema = original_table.schema

# Step 2: Create a DataFrame with Partition Dates
start_date = datetime.datetime(2022, 1, 1)
end_date = datetime.datetime(2022, 1, 10)  # Adjust the end date as needed
partition_dates = pd.date_range(start=start_date, end=end_date, freq='D')

# Step 3: Repeat Original Content for Each Partition Date
repeated_data = []

for partition_date in partition_dates:
    for row in original_table.iterrows():
        original_record = row[1].as_py()

        # Include the partition_date
        original_record['partition_date'] = partition_date
        repeated_data.append(original_record)

# Create a Pandas DataFrame from the repeated data
repeated_df = pd.DataFrame(repeated_data)

# Step 4: Write the Repeated Data to a Parquet File
repeated_table = pa.Table.from_pandas(repeated_df)

# Write the repeated data to a local Parquet file
pq.write_table(repeated_table, 'repeated_data.parquet')

# Step 5: Upload the Parquet File to S3 with Partitioning
bucket_name = 'your-s3-bucket-name'
s3_key = 'path/to/destination/'

# Create partitions based on 'partition_date'
for partition_date in partition_dates:
    partitioned_s3_key = f'{s3_key}partition_date={partition_date.strftime("%Y-%m-%d")}/'
    s3.upload_file('repeated_data.parquet', bucket_name, f'{partitioned_s3_key}customer_sub_file.parquet')

print(f'Repeated data uploaded to S3 with partitioning: s3://{bucket_name}/{s3_key}')
