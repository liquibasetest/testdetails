from pyspark.context import SparkContext
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("example").getOrCreate()

# Read Parquet file into a DataFrame
input_file_path = "s3://your-bucket/input_file.parquet"
df = spark.read.parquet(input_file_path)

# Modify the DataFrame (replace this with your own modifications)
df = df.withColumn('new_column', df['existing_column'] * 2)

# Write the modified DataFrame to a new Parquet file
output_file_path = "s3://your-bucket/output_file.parquet"
df.write.parquet(output_file_path, mode='overwrite')

# Stop the SparkSession
spark.stop()
