https://github.com/developer-advocacy-dremio/quick-guides-from-dremio/tree/main

# Check if the Iceberg table exists
if not spark._jsparkSession.catalog().tableExists(iceberg_output_path):
    # If the table does not exist, create it
    df.write.format("iceberg").mode("overwrite").option("uri", iceberg_output_path).saveAsTable("default.my_iceberg_table")
else:
    # If the table exists, append data to it
    df.write.format("iceberg").mode("append").option("uri", iceberg_output_path).saveAsTable("default.my_iceberg_table")


from awsglue.context import GlueContext
from pyspark.context import SparkContext
from awsglue.dynamicframe import DynamicFrame

# Create a GlueContext and SparkContext
glueContext = GlueContext(SparkContext.getOrCreate())

# Specify the paths
parquet_input_path = "s3://your-bucket/input_file.parquet"
iceberg_output_path = "s3://your-bucket/output_iceberg_table"

# Read Parquet data into a DynamicFrame
dynamic_frame = glueContext.create_dynamic_frame.from_catalog(database="your_database", table_name="your_parquet_table")

# Write DynamicFrame to Iceberg table
glueContext.write_dynamic_frame.from_catalog(
    frame=dynamic_frame,
    database="default",
    table_name="my_iceberg_table",
    transformation_ctx="datasink0",
    format="iceberg",
    format_options={"write.operation": "overwrite"},
    catalog_versioning=False,  # Optional, set to True if you want to enable catalog versioning
    additional_options={"write.location": iceberg_output_path}  # Specify the Iceberg output path
)

