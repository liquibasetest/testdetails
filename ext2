write_options = pa.parquet.ParquetWriteOptions(compression='SNAPPY')

# Write the fake data to a local Parquet file with Snappy compression
pq.write_table(fake_table, 'fake_data.parquet', write_options=write_options)

# Step 5: Upload the Parquet File to S3 with Partitioning
bucket_name = 'your-s3-bucket-name'
s3_key = 'path/to/destination/'

s3 = boto3.resource('s3')  # Create an S3 resource
bucket = s3.Bucket(bucket_name)  # Specify the bucket

# Create partitions based on 'partition_date'
for partition_date in random_partition_dates:
    partitioned_s3_key = f'{s3_key}partition_date={partition_date.strftime("%Y-%m-%d")}/customer_sub_file.parquet'
    bucket.upload_file('fake_data.parquet', partitioned_s3_key)
