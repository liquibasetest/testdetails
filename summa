During the meeting, we discussed the transition of DIS-DQCS from an on-premise setup to the AWS Cloud environment. 

Key items discussed:

DIS-DQCS Migration Approach
Reason for Considering Few AWS Services. ECS-Fargate, Postgres
Backup Region Approach Considering the RTO
Enhancement of Data transmission capture in DQCS

Approval:  Paoli provided the approval for the WAF document and earlier Todd provide his approval. 

Key takeaways and Action taken.

Suggestion to include all applications in both the consumer risk platform and the CCB Risk strategy in the product overview page. To address this, changes were made in the presentation slides (PPT). The updated version has been attached for reference.

Backup region (Pilotlight vs Backup and Restore)
A significant aspect of the discussion was the consideration of DIS-abc's current RTO, which is less than 4 hours. Pilot light option in the AWS Backup region was suggested as a potential solution. The attendees decided to verify the cost comparison between the backup and restore approach and the pilot light option. 

Action taken: It has been observed that the cost, including the pilot light, is more economical compared to the current on-premise setup. So modifications made in the backup region to
operate in the pilot light mode. Further discussions with the business team will be held to explain the cost difference. 

Absence of data transmission control capture in DIS-DQCS. The team acknowledged the significance of this feature and its potential benefits. To address this concern, it was agreed that the implementation of data transmission control capture will be considered in future as part of DIS-DQCS. 


Chatbot for TPS, RAS, and AIS Products:

Creation of chatbot to facilitate information exchange within the TPS, RAS, and AIS team. The chatbot's purpose is to collect questions related to products, applications, and technical details from various teams and provide appropriate answers from the respective teams. The Minimum Viable Product (MVP) target is to have a chatbot capable of answering 10 questions for each application, categorized into technical and business aspects.

Steps to Implement:
a. Requirement Gathering: Identify the specific information needs and questions from the teams in the TPS, RAS, and AIS Forum. Categorize these questions into technical and business domains.
b. Data Collection: Gather relevant information and responses from the respective teams for each application. This data will serve as the knowledge base for the chatbot.
c. Chatbot Development: Development of the chatbot application which cabable of answering 10 questions. 

Benefits: Efficient Information Exchange, Time and Resource Savings, Enhanced Knowledge Managemen

Data Flow Diagram Generation Chatbot:

Creation of chatbot capable of generating data flow diagrams based on application input, output details, and functionalities. The chatbot will use the collected data from various LOB (Line of Business) levels to depict data flow, identifying whether the data is located on-premise or in the cloud.
Steps to Implement:
a. Data Collection: Gather detailed information about applications, including input, output, and functionality details. Also, collect information about data locations (on-premise or cloud) for each application.
b. Data Flow Mapping: Develop a data flow mapping mechanism to understand how data moves within applications and systems.
c. Chatbot Development: Development of the chatbot application

Benefits:Visual Representation of Data Flows, Streamlined Data Mapping, Faster Decision-making


Common Data Extract framework (If not already in place)

Proposal involves defining a common extract framework that allows queries to run against Athena/Redshift Spectrum, producing outputs in various formats such as CSV, PDF, XLS, and JSON for weekly/monthly reporting purposes.

Benefits: Standardized Reporting, Avoid Multiple extract jobs/process for extracting from athena



During the meeting, A and B discussed the approach for the upcoming tech forum and shared their thoughts. It was agreed to proceed by identifying potential problems and presenting them in the tech forum for further discussion. Additionally, solution ideas will be brainstormed and discussed collectively. Once a successful solution is implemented in one application, it can be applied across all teams for better standardization and efficiency.

It has been decided that the progress of several existing tech forum subjects needs to be properly updated. Suresh will follow up with the teams, update the progress, and prioritize the items in the next meeting.

In the current techforum items,best tech idea to be adopted across team is "Aurora Postgres load using Glue." This proposal can be presented in the next week's techforum, so the other teams aware of this implementation.

Currently, the LF Tag-based Lake formation approach is not followed due to the unavailability of the corresponding blueprint. Once the blueprint with LF tag-based capability becomes available, the existing data catalog resource-based approach will be updated, making it a potential use case for the techforum.

