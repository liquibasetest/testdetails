from pyspark.sql import SparkSession
from delta import DeltaTable

# Create a Spark session
spark = SparkSession.builder.appName("Convert Parquet to Delta").getOrCreate()

# Read the Parquet file with partitioning
parquetDF = spark.read.format("parquet").load("path/to/parquet/directory")

# Convert to Delta Lake table and maintain partitioning
parquetDF.write.format("delta").partitionBy("partition_field").save("path/to/delta/table")

# Manage the Delta table using DeltaTable API
delta_table = DeltaTable.forPath(spark, "path/to/delta/table")

# You can now use delta_table for managing your Delta Lake table, e.g., appending data, updating, querying, etc.


from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("View Delta Table").getOrCreate()

# Read the Delta Lake table
delta_path = "path/to/delta/table"
delta_df = spark.read.format("delta").load(delta_path)

# Show the first few rows
delta_df.show()
