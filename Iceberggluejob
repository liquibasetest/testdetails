import sys
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext

# Initialize Spark and Glue contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init('ParquetToLakeFormationTableJob')

# Specify source and target S3 paths
source_path = 's3://your-source-bucket/source-path/'  # Replace with your actual source path
database_name = 'your_database_name'  # Replace with your actual Lake Formation database name
table_name = 'your_table_name'  # Replace with your desired Glue table name
target_path = f's3://your-target-bucket/target-path/{table_name}/'  # Replace with your actual target path

# Create a DynamicFrame for the source Parquet data
source_dyf = glueContext.create_dynamic_frame.from_catalog(
    database=None,
    table_name=None,
    path=source_path
)

# Infer the schema and create a new Glue table in Lake Formation
target_dyf = glueContext.create_dynamic_frame.from_catalog(
    database=database_name,
    table_name=table_name,
    path=target_path,
    transformation_ctx='target_dyf'
)

# Print the schema of the target DynamicFrame (optional)
target_dyf.printSchema()

# Commit the job
job.commit()
