import pandas as pd
from faker import Faker
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from delta import DeltaTable

# Initialize Faker to generate fake data
fake = Faker()

# Generate random data for 50 unique business dates, each with one million records
data = {
    'customer_id': [],
    'frst_nm': [],
    'last_nm': [],
    'dob_dt': [],
    'crt_crd_num': [],
    'regist_dt': [],
    'addr': [],
    'email_addr': [],
    'phne_nm': [],
    'city_nm': [],
    'cntry_nm': [],
    'create_tp': [],
    'bus_tp': []
}

# Generate data for 50 business dates
start_date = datetime.now() - timedelta(days=365)
end_date = datetime now()
for _ in range(50):
    for _ in range(20000):  # 1 million records / 50 dates
        data['customer_id'].append(fake.unique.random_int(min=1, max=1000000, step=1))
        data['frst_nm'].append(fake.first_name())
        data['last_nm'].append(fake.last_name())
        data['dob_dt'].append(fake.date_of_birth(minimum_age=18, maximum_age=90).strftime('%Y-%m-%d'))
        data['crt_crd_num'].append(fake.credit_card_number(card_type=None))
        data['regist_dt'].append(fake.date_between(start_date=start_date, end_date=end_date).strftime('%Y-%m-%d'))
        data['addr'].append(fake.address())
        data['email_addr'].append(fake.email())
        data['phne_nm'].append(fake.phone_number())
        data['city_nm'].append(fake.city())
        data['cntry_nm'].append(fake.country())
        data['create_tp'].append(datetime.now().strftime('%Y-%m-%d %H:M:S'))
        data['bus_tp'].append(fake.date_between(start_date=start_date, end_date=end_date).strftime('%Y-%m-%d'))

# Create a DataFrame from the data
df = pd.DataFrame(data)

# Initialize a Spark session
spark = SparkSession.builder \
    .appName("Delta Lake Writer") \
    .getOrCreate()

# Convert the Pandas DataFrame to a Spark DataFrame
spark_df = spark.createDataFrame(df)

# Write the DataFrame to the Delta Lake table
(
    spark_df
    .repartition("bus_tp")
    .write
    .format("delta")
    .mode("overwrite")
    .save("s3a://samplebucket/customerdeltalake")
)

# Compact the Delta table (optional but can improve query performance)
delta_table = DeltaTable.forPath(spark, "s3a://samplebucket/customerdeltalake")
delta_table.vacuum(0)

# Stop the Spark session
spark.stop()

print("Data written to Delta Lake format in 's3://samplebucket/customerdeltalake'.")
