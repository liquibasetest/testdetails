import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from faker import Faker
from datetime import datetime
import os
import boto3

# AWS access key and secret key
aws_access_key_id = 'your_access_key'
aws_secret_access_key = 'your_secret_key'

# Initialize Faker to generate fake data
fake = Faker()

# Generate random data for 1 million records
data = {
    'customer_id': range(1, 1000001),
    'frst_nm': [fake.first_name() for _ in range(1000000)],
    'last_nm': [fake.last_name() for _ in range(1000000)],
    'dob_dt': [fake.date_of_birth(minimum_age=18, maximum_age=90).strftime('%Y-%m-%d') for _ in range(1000000)],
    'crt_crd_num': [fake.credit_card_number(card_type=None) for _ in range(1000000)],
    'regist_dt': [fake.date_between(start_date='-1y', end_date='today').strftime('%Y-%m-%d') for _ in range(1000000),
    'addr': [fake.address() for _ in range(1000000)],
    'email_addr': [fake.email() for _ in range(1000000)],
    'phne_nm': [fake.phone_number() for _ in range(1000000)],
    'city_nm': [fake.city() for _ in range(1000000)],
    'cntry_nm': [fake.country() for _ in range(1000000)],
    'create_tp': [datetime.now().strftime('%Y-%m-%d %H:%M:%S') for _ in range(1000000)],
    'bus_tp': [fake.date_between(start_date='-1y', end_date='today').strftime('%Y-%m-%d') for _ in range(1000000)]
}

# Create a DataFrame from the data
df = pd.DataFrame(data)

# Ensure 'bus_tp' is in datetime format
df['bus_tp'] = pd.to_datetime(df['bus_tp'])

# Create a directory to store the Parquet files
output_dir = 'parquet_output'
os.makedirs(output_dir, exist_ok=True)

# Write the DataFrame to a Parquet file partitioned by 'bus_tp'
table = pa.Table.from_pandas(df)

# Write the Parquet file and partition by 'bus_tp'
pq.write_to_dataset(
    table,
    root_path=output_dir,
    partition_cols=['bus_tp'],
    filesystem=None,
    use_legacy_dataset=False
)

# Upload the Parquet files to Amazon S3
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
bucket_name = 's3buckettest'
s3_key_prefix = 'tpa/ace1/'

# Upload files to S3
for root, dirs, files in os.walk(output_dir):
    for file in files:
        s3.upload_file(os.path.join(root, file), bucket_name, f'{s3_key_prefix}{file}')

print(f"Parquet files uploaded to S3 bucket: s3://{bucket_name}/{s3_key_prefix}")

